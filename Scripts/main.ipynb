{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n",
      "Target dtype: float64\n",
      "Unique values: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import data_preprocessor as dp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 1. Load the dataset\n",
    "messy_data = pd.read_csv(\"C:/Users/rabiy/BINF-5507-Materials-ASSIGNMENT-1/Data/messy_data.csv\")\n",
    "clean_data = messy_data.copy()\n",
    "\n",
    "\n",
    "# 2. Preprocess the data\n",
    "# a .  Impute missing values\n",
    "def impute_missing_values(data, strategy='mean'):\n",
    "    \"\"\"Fill missing values in the dataset.\"\"\"\n",
    "    if strategy == 'mean':\n",
    "        data = data.fillna(data.mean(numeric_only=True))\n",
    "    elif strategy == 'median':\n",
    "        data = data.fillna(data.median(numeric_only=True))\n",
    "    elif strategy == 'mode':\n",
    "        data = data.fillna(data.mode().iloc[0])\n",
    "    return data\n",
    "\n",
    "# b. Remove duplicates\n",
    "def remove_duplicates(data):\n",
    "    \"\"\"Remove duplicate rows from the dataset.\"\"\"\n",
    "    data = data.drop_duplicates()\n",
    "    return data\n",
    "\n",
    "# c. Normalize numerical features\n",
    "def normalize_data(data, method='minmax'):\n",
    "    \"\"\"Apply normalization to numerical features.\"\"\"\n",
    "    numerical_features = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    data[numerical_features] = scaler.fit_transform(data[numerical_features])\n",
    "    return data\n",
    "\n",
    "# d. Remove highly correlated features\n",
    "def remove_redundant_features(data, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Remove highly correlated numeric features.\n",
    "    \n",
    "    :param data: pandas DataFrame\n",
    "    :param threshold: float, correlation threshold\n",
    "    :return: pandas DataFrame with redundant features removed\n",
    "    \"\"\"\n",
    "    data_copy = data.copy()\n",
    "    numeric_data = data_copy.select_dtypes(include=[np.number])  # Only numeric columns\n",
    "\n",
    "    corr_matrix = numeric_data.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "    return data_copy.drop(columns=to_drop, errors='ignore')  # Drop from original data, not just numeric\n",
    "\n",
    "# 3. Save the cleaned dataset\n",
    "clean_data.to_csv('../Data/clean_data.csv', index=False)\n",
    "\n",
    "\n",
    "# 4. Train and evaluate the model\n",
    "dp.simple_model(clean_data)\n",
    "def simple_model(input_data, split_data=True, scale_data=False, print_report=False):\n",
    "    \"\"\"\n",
    "    A simple logistic regression model for target classification.\n",
    "    Parameters:\n",
    "    input_data (pd.DataFrame): The input data containing features and the target variable 'target' (assume 'target' is the first column).\n",
    "    split_data (bool): Whether to split the data into training and testing sets. Default is True.\n",
    "    scale_data (bool): Whether to scale the features using StandardScaler. Default is False.\n",
    "    print_report (bool): Whether to print the classification report. Default is False.\n",
    "    Returns:\n",
    "    None\n",
    "    The function performs the following steps:\n",
    "    1. Removes columns with missing data.\n",
    "    2. Splits the input data into features and target.\n",
    "    3. Encodes categorical features using one-hot encoding.\n",
    "    4. Splits the data into training and testing sets (if split_data is True).\n",
    "    5. Scales the features using StandardScaler (if scale_data is True).\n",
    "    6. Instantiates and fits a logistic regression model.\n",
    "    7. Makes predictions on the test set.\n",
    "    8. Evaluates the model using accuracy score and classification report.\n",
    "    9. Prints the accuracy and classification report (if print_report is True).\n",
    "    \"\"\"\n",
    "\n",
    "# if there's any missing data, remove the columns\n",
    "clean_data.dropna(inplace=True)\n",
    "target = clean_data[clean_data.columns[0]]\n",
    "print(\"Target dtype:\", target.dtype)\n",
    "print(\"Unique values:\", target.unique())\n",
    "\n",
    "# If target is continuous but classification is intended, discretize or encode:\n",
    "# Example for binary classification:\n",
    "if target.dtype != 'object' and len(target.unique()) > 10:\n",
    "    print(\"Target looks continuous; consider converting to categorical labels.\")\n",
    "\n",
    "\n",
    "    # split the data into features and target\n",
    "    target = input_data.copy()[input_data.columns[0]]\n",
    "    features = input_data.copy()[input_data.columns[1:]]\n",
    "\n",
    "    # if the column is not numeric, encode it (one-hot)\n",
    "    for col in features.columns:\n",
    "        if features[col].dtype == 'object':\n",
    "            features = pd.concat([features, pd.get_dummies(features[col], prefix=col)], axis=1)\n",
    "            features.drop(col, axis=1, inplace=True)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, stratify=target, random_state=42)\n",
    "\n",
    "    if scale_data:\n",
    "        # scale the data\n",
    "        X_train = normalize_data(X_train)\n",
    "        X_test = normalize_data(X_test)\n",
    "        \n",
    "    # instantiate and fit the model\n",
    "    log_reg = LogisticRegression(random_state=42, max_iter=100, solver='liblinear', penalty='l2', C=1.0)\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions and evaluate the model\n",
    "    y_pred = log_reg.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n",
      "Target dtype: float64\n",
      "Unique values: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import data_preprocessor as dp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 1. Load the dataset\n",
    "messy_data = pd.read_csv(\"C:/Users/rabiy/BINF-5507-Materials-ASSIGNMENT-1/Data/messy_data.csv\")\n",
    "clean_data = messy_data.copy()\n",
    "\n",
    "\n",
    "# 2. Preprocess the data\n",
    "def impute_missing_values(data, strategy='mean'):\n",
    "    \"\"\"Fill missing values in the dataset.\"\"\"\n",
    "    if strategy == 'mean':\n",
    "        data = data.fillna(data.mean(numeric_only=True))\n",
    "    elif strategy == 'median':\n",
    "        data = data.fillna(data.median(numeric_only=True))\n",
    "    elif strategy == 'mode':\n",
    "        data = data.fillna(data.mode().iloc[0])\n",
    "    return data\n",
    "\n",
    "def remove_duplicates(data):\n",
    "    \"\"\"Remove duplicate rows from the dataset.\"\"\"\n",
    "    data = data.drop_duplicates()\n",
    "    return data\n",
    "\n",
    "def normalize_data(data, method='minmax'):\n",
    "    \"\"\"Apply normalization to numerical features.\"\"\"\n",
    "    numerical_features = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    data[numerical_features] = scaler.fit_transform(data[numerical_features])\n",
    "    return data\n",
    "\n",
    "def remove_redundant_features(data, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Remove highly correlated numeric features.\n",
    "    \n",
    "    :param data: pandas DataFrame\n",
    "    :param threshold: float, correlation threshold\n",
    "    :return: pandas DataFrame with redundant features removed\n",
    "    \"\"\"\n",
    "    data_copy = data.copy()\n",
    "    numeric_data = data_copy.select_dtypes(include=[np.number])  # Only numeric columns\n",
    "\n",
    "    corr_matrix = numeric_data.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "    return data_copy.drop(columns=to_drop, errors='ignore')  # Drop from original data, not just numeric\n",
    "\n",
    "# 3. Save the cleaned dataset\n",
    "clean_data.to_csv('../Data/clean_data.csv', index=False)\n",
    "\n",
    "\n",
    "# 4. Train and evaluate the model\n",
    "dp.simple_model(clean_data)\n",
    "def simple_model(input_data, split_data=True, scale_data=False, print_report=False):\n",
    "    \"\"\"\n",
    "    A simple logistic regression model for target classification.\n",
    "    Parameters:\n",
    "    input_data (pd.DataFrame): The input data containing features and the target variable 'target' (assume 'target' is the first column).\n",
    "    split_data (bool): Whether to split the data into training and testing sets. Default is True.\n",
    "    scale_data (bool): Whether to scale the features using StandardScaler. Default is False.\n",
    "    print_report (bool): Whether to print the classification report. Default is False.\n",
    "    Returns:\n",
    "    None\n",
    "    The function performs the following steps:\n",
    "    1. Removes columns with missing data.\n",
    "    2. Splits the input data into features and target.\n",
    "    3. Encodes categorical features using one-hot encoding.\n",
    "    4. Splits the data into training and testing sets (if split_data is True).\n",
    "    5. Scales the features using StandardScaler (if scale_data is True).\n",
    "    6. Instantiates and fits a logistic regression model.\n",
    "    7. Makes predictions on the test set.\n",
    "    8. Evaluates the model using accuracy score and classification report.\n",
    "    9. Prints the accuracy and classification report (if print_report is True).\n",
    "    \"\"\"\n",
    "\n",
    "# if there's any missing data, remove the columns\n",
    "clean_data.dropna(inplace=True)\n",
    "target = clean_data[clean_data.columns[0]]\n",
    "print(\"Target dtype:\", target.dtype)\n",
    "print(\"Unique values:\", target.unique())\n",
    "\n",
    "# If target is continuous but classification is intended, discretize or encode:\n",
    "# Example for binary classification:\n",
    "if target.dtype != 'object' and len(target.unique()) > 10:\n",
    "    print(\"Target looks continuous; consider converting to categorical labels.\")\n",
    "\n",
    "\n",
    "    # split the data into features and target\n",
    "    target = input_data.copy()[input_data.columns[0]]\n",
    "    features = input_data.copy()[input_data.columns[1:]]\n",
    "\n",
    "    # if the column is not numeric, encode it (one-hot)\n",
    "    for col in features.columns:\n",
    "        if features[col].dtype == 'object':\n",
    "            features = pd.concat([features, pd.get_dummies(features[col], prefix=col)], axis=1)\n",
    "            features.drop(col, axis=1, inplace=True)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, stratify=target, random_state=42)\n",
    "\n",
    "    if scale_data:\n",
    "        # scale the data\n",
    "        X_train = normalize_data(X_train)\n",
    "        X_test = normalize_data(X_test)\n",
    "        \n",
    "    # instantiate and fit the model\n",
    "    log_reg = LogisticRegression(random_state=42, max_iter=100, solver='liblinear', penalty='l2', C=1.0)\n",
    "    log_reg.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions and evaluate the model\n",
    "    y_pred = log_reg.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(f'Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
